{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarwazulkar/GoogleColab/blob/main/H4NoRobots_FineTune_Llama3_1B_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CBzNuEcdQOV"
      },
      "source": [
        "## **Reqs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xghl4diPvuJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U trl bitsandbytes transformers accelerate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ-eYAoBNfPO"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-4pOo11RWoG"
      },
      "outputs": [],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RiBSAgZafXl"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy9csO2iagd4"
      },
      "outputs": [],
      "source": [
        "system_message = \"\"\"You are Llama, an AI assistant created by Omkar to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tdVrRDdah73"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"HuggingFaceH4/no_robots\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfC_qC1Baji2"
      },
      "outputs": [],
      "source": [
        "columns_to_remove = [\n",
        "    c for c in dataset[\"train\"].column_names if c != \"messages\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3QLtRjPak55"
      },
      "outputs": [],
      "source": [
        "def create_conversation(sample):\n",
        "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
        "        return {\"messages\": sample[\"messages\"]}\n",
        "    else:\n",
        "        return {\n",
        "            \"messages\": [{\"role\": \"system\", \"content\": system_message}]\n",
        "            + sample[\"messages\"]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbYDWlo9amK3"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(\n",
        "    create_conversation,\n",
        "    remove_columns=columns_to_remove,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy-LedZ4anN5"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"] = dataset[\"train\"].filter(\n",
        "    lambda x: len(x[\"messages\"][1:]) % 2 == 0\n",
        ")\n",
        "dataset[\"test\"] = dataset[\"test\"].filter(\n",
        "    lambda x: len(x[\"messages\"][1:]) % 2 == 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xekn8cAaoXn"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj2J7ATxarwP"
      },
      "outputs": [],
      "source": [
        "LLAMA_3_CHAT_TEMPLATE = (\n",
        "    \"{% for message in messages %}\"\n",
        "    \"{% if message['role'] == 'system' %}\"\n",
        "    \"{{ message['content'] }}\"\n",
        "    \"{% elif message['role'] == 'user' %}\"\n",
        "    \"{{ '\\\\n\\\\nHuman: ' + message['content'] + eos_token }}\"\n",
        "    \"{% elif message['role'] == 'assistant' %}\"\n",
        "    \"{{ '\\\\n\\\\nAssistant: ' + message['content'] + eos_token }}\"\n",
        "    \"{% endif %}\"\n",
        "    \"{% endfor %}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CzneGBGat5Y"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    use_fast=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc3afnMfa4Cz"
      },
      "outputs": [],
      "source": [
        "def template_dataset(example):\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            example[\"messages\"],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f854RyCbaGx1"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    template_dataset, remove_columns=[\"messages\"]\n",
        ")\n",
        "test_dataset = test_dataset.map(\n",
        "    template_dataset, remove_columns=[\"messages\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8WyugsI50E"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[1][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t9pTOvANl4y"
      },
      "source": [
        "## **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66DKYfpLYzMU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opS-sKagbGta"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16,\n",
        "    use_cache=False,\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpOzoczTbRSs"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=\"all-linear\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5LrlfuTbUyG"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/llama3.2-1b-lora\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "\n",
        "    # ðŸ”’ T4 Safe\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    tf32=False,\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"tensorboard\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdowALg4baRI"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "trainer.model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u72nqDEbuPr"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrHPpltXl0Z3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "lora_path = \"/content/llama3.2-1b-lora/checkpoint-1186\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(lora_path, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDAgHXlNmCMM"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_path,\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi-sAJ8tmHSs"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are Llama, an AI assistant created by Omkar to be helpful and honest.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain gradient checkpointing in simple terms.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aA_5tWXmJqw"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDLaQNVimK75"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di6ycXbPmQMI"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsmR6QJXmShB"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-cfvSppQh_1"
      },
      "outputs": [],
      "source": [
        "response = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYGkzxm9mWg3"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j_iYXmMcjAR"
      },
      "source": [
        "## **Push LoRA to HF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBLx3eLLm-Oj"
      },
      "outputs": [],
      "source": [
        "hf_repo = \"omkarwazulkar/Llama-3.2-1B-LoRA-HuggingFaceH4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFCKD1KknkuE"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(hf_repo)\n",
        "tokenizer.push_to_hub(hf_repo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdL-yvzdEX8"
      },
      "source": [
        "## **Inference LoRA Adapter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMuqtAR5ZZ-h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2trrnYmWIZx"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "lora_path = \"omkarwazulkar/Llama-3.2-1B-LoRA-HuggingFaceH4\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(lora_path, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s8dR-YFW0H6"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    lora_path,\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiHzNL1TWoiw"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are Llama, an AI assistant created by Omkar to be helpful and honest.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain gradient checkpointing in simple terms.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfXj6J_9Wr2T"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLCLQXQIXLr-"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkYBgBTxXHTj"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvAG4taccsXQ"
      },
      "source": [
        "## **Push Merged to HF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NvwNgxqmoDm"
      },
      "outputs": [],
      "source": [
        "merged = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJvXZXqlX8j_"
      },
      "outputs": [],
      "source": [
        "repo_id = \"omkarwazulkar/Merged-Llama-3.2-1B-LoRA-HuggingFaceH4\"\n",
        "\n",
        "merged.push_to_hub(\n",
        "    repo_id,\n",
        "    private=False,\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    repo_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsiLM8B0c0wY"
      },
      "source": [
        "## **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHnWEU2oc8U-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYMeoG82ZDQP"
      },
      "outputs": [],
      "source": [
        "repo_id = \"omkarwazulkar/Merged-Llama-3.2-1B-LoRA-HuggingFaceH4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox4p3OenZID_"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(repo_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    repo_id,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO6UdjCvbpeL"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are Llama, an AI assistant created by Omkar to be helpful and honest.\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain gradient checkpointing in simple terms.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNeTNyhybuDG"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocqJtsmWbwTS"
      },
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF3YGrT3bxoa"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
